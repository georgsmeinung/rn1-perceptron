{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBPoiXRPDonZiXjFbtge11",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/georgsmeinung/rn1-perceptron/blob/main/RN_Clase_3_Actividad_en_Clase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SingleNeuronMLP:\n",
        "    \"\"\"\n",
        "    Implementa una única neurona (Perceptrón) con activación sigmoide\n",
        "    y pesos fijos.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Pesos (w) extraídos de la captura de pantalla\n",
        "        # W(Area), W(Perimetro), W(Compactidad), W(LongNucleo),\n",
        "        # W(AnchoNucleo), W(Asimetria), W(LongSurco)\n",
        "        self.weights = np.array([\n",
        "            0.58,\n",
        "            -0.43,\n",
        "            -0.19,\n",
        "            -0.47,\n",
        "            -0.009,\n",
        "            -0.75,\n",
        "            -0.11\n",
        "        ])\n",
        "\n",
        "        # Sesgo (b) extraído de la captura\n",
        "        self.bias = 0.039\n",
        "\n",
        "        # Nombres de las características para una salida más clara\n",
        "        self.feature_names = [\n",
        "            \"W(Area)\", \"W(Perimetro)\", \"W(Compactidad)\", \"W(LongNucleo)\",\n",
        "            \"W(AnchoNucleo)\", \"W(Asimetria)\", \"W(LongSurco)\"\n",
        "        ]\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        \"\"\"Función de activación sigmoide.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def _sigmoid_derivative(self, y_pred):\n",
        "        \"\"\"Derivada de la función sigmoide (y_pred * (1 - y_pred)).\"\"\"\n",
        "        return y_pred * (1 - y_pred)\n",
        "\n",
        "    def forward_pass(self, x):\n",
        "        \"\"\"\n",
        "        Realiza el 'forward pass' calculando la entrada neta (z)\n",
        "        y la salida activada (y_pred).\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Vector de entrada con 7 características.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (y_pred, z)\n",
        "                   y_pred: La salida de la neurona (predicción).\n",
        "                   z: La entrada neta (combinación lineal).\n",
        "        \"\"\"\n",
        "        # 1. Calcular la combinación lineal (entrada neta)\n",
        "        # z = (w · x) + b\n",
        "        z = np.dot(self.weights, x) + self.bias\n",
        "\n",
        "        # 2. Aplicar la función de activación\n",
        "        # y_pred = sigmoid(z)\n",
        "        y_pred = self._sigmoid(z)\n",
        "\n",
        "        return y_pred, z\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Calcula la salida (predicción) de la neurona para un vector de entrada x.\n",
        "        \"\"\"\n",
        "        # Solo necesitamos la predicción, no z\n",
        "        y_pred, _ = self.forward_pass(x)\n",
        "        return y_pred\n",
        "\n",
        "    def calculate_cost(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calcula el valor de la función de costo (Error Cuadrático Medio)\n",
        "        para una sola muestra.\n",
        "\n",
        "        C = (y_true - y_pred)^2\n",
        "        \"\"\"\n",
        "        cost = (y_true - y_pred)**2\n",
        "        return cost\n",
        "\n",
        "    def calculate_gradients(self, x, y_true):\n",
        "        \"\"\"\n",
        "        Calcula los gradientes de la función de costo (MSE) con respecto\n",
        "        a los pesos (w) y el sesgo (b) para una sola muestra (x, y_true).\n",
        "\n",
        "        Utiliza la regla de la cadena:\n",
        "        dC/dw = dC/dy_pred * dy_pred/dz * dz/dw\n",
        "        dC/db = dC/dy_pred * dy_pred/dz * dz/db\n",
        "        \"\"\"\n",
        "\n",
        "        # --- 1. Forward Pass ---\n",
        "        # Necesitamos la predicción (y_pred) y la entrada neta (z)\n",
        "        y_pred, z = self.forward_pass(x)\n",
        "\n",
        "        # --- 2. Backward Pass (Cálculo de derivadas) ---\n",
        "\n",
        "        # Parte 1: dC/dy_pred (Derivada del costo MSE)\n",
        "        # C = (y_true - y_pred)^2  =>  dC/dy_pred = -2 * (y_true - y_pred)\n",
        "        dC_dy_pred = -2 * (y_true - y_pred)\n",
        "\n",
        "        # Parte 2: dy_pred/dz (Derivada de la sigmoide)\n",
        "        # dy_pred/dz = sigmoid(z) * (1 - sigmoid(z)) = y_pred * (1 - y_pred)\n",
        "        dy_pred_dz = self._sigmoid_derivative(y_pred)\n",
        "\n",
        "        # Combinamos las partes comunes (delta de error)\n",
        "        # delta = dC/dy_pred * dy_pred/dz\n",
        "        delta = dC_dy_pred * dy_pred_dz\n",
        "\n",
        "        # --- 3. Cálculo de Gradientes Finales ---\n",
        "\n",
        "        # Parte 3a: dz/dw (Derivada de la entrada neta w.r.t. pesos)\n",
        "        # z = w1*x1 + w2*x2 + ... + b  =>  dz/dw_i = x_i\n",
        "        # dC/dw = delta * x\n",
        "        grad_weights = delta * x\n",
        "\n",
        "        # Parte 3b: dz/db (Derivada de la entrada neta w.r.t. sesgo)\n",
        "        # z = ... + b  =>  dz/db = 1\n",
        "        # dC/db = delta * 1\n",
        "        grad_bias = delta\n",
        "\n",
        "        return grad_weights, grad_bias\n",
        "\n",
        "# --- Ejemplo de uso ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 1. Instanciar la neurona con los pesos fijos\n",
        "    neuron = SingleNeuronMLP()\n",
        "\n",
        "    # 2. Crear una fila de datos de ejemplo (un nuevo 'row' del dataset)\n",
        "    # Debe tener 7 características, una para cada peso.\n",
        "    # [Area, Perimetro, Compactidad, LongNucleo, AnchoNucleo, Asimetria, LongSurco]\n",
        "    x_sample = np.array([15.03, 14.77, 0.8658, 5.702, 3.212, 1.933, 5.439])\n",
        "\n",
        "    # 3. Crear una etiqueta verdadera de ejemplo (para calcular costo y gradiente)\n",
        "    # Como la salida es sigmoide, la etiqueta real suele ser 0 o 1.\n",
        "    y_sample = 0\n",
        "\n",
        "    print(\"--- Neurona MLP de Carga Única ---\")\n",
        "    print(f\"Pesos (w): {neuron.weights}\")\n",
        "    print(f\"Sesgo (b):   {neuron.bias}\\n\")\n",
        "    print(f\"Entrada (x): {x_sample}\")\n",
        "    print(f\"Etiqueta (y): {y_sample}\")\n",
        "    print(\"-\" * 35)\n",
        "\n",
        "    # --- Tarea 1: Calcular la salida ---\n",
        "    print(\"\\n## 1. Calcular Salida (Predicción) ##\")\n",
        "    prediction = neuron.predict(x_sample)\n",
        "    print(f\"La predicción (y_pred) es: {prediction:.6f}\")\n",
        "\n",
        "    # --- Tarea 2: Calcular el valor de la función de costo ---\n",
        "    print(\"\\n## 2. Calcular Función de Costo (MSE) ##\")\n",
        "    cost = neuron.calculate_cost(y_sample, prediction)\n",
        "    print(f\"El costo (C = (y - y_pred)^2) es: {cost:.6f}\")\n",
        "\n",
        "    # --- Tarea 3: Calcular el gradiente ---\n",
        "    print(\"\\n## 3. Calcular Gradientes (dC/dw, dC/db) ##\")\n",
        "    grad_w, grad_b = neuron.calculate_gradients(x_sample, y_sample)\n",
        "\n",
        "    print(\"Gradientes de los Pesos (dC/dw):\")\n",
        "    for i, name in enumerate(neuron.feature_names):\n",
        "        print(f\"  {name:>15}: {grad_w[i]:.6f}\")\n",
        "\n",
        "    print(f\"\\nGradiente del Sesgo (dC/db):\\n  b: {grad_b:.6f}\")"
      ],
      "metadata": {
        "id": "EUCgf5c8DJuS",
        "outputId": "d404a5c1-d496-417d-eeb1-e3b6e6daa9b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Neurona MLP de Carga Única ---\n",
            "Pesos (w): [ 0.58  -0.43  -0.19  -0.47  -0.009 -0.75  -0.11 ]\n",
            "Sesgo (b):   0.039\n",
            "\n",
            "Entrada (x): [15.03   14.77    0.8658  5.702   3.212   1.933   5.439 ]\n",
            "Etiqueta (y): 0\n",
            "-----------------------------------\n",
            "\n",
            "## 1. Calcular Salida (Predicción) ##\n",
            "La predicción (y_pred) es: 0.074738\n",
            "\n",
            "## 2. Calcular Función de Costo (MSE) ##\n",
            "El costo (C = (y - y_pred)^2) es: 0.005586\n",
            "\n",
            "## 3. Calcular Gradientes (dC/dw, dC/db) ##\n",
            "Gradientes de los Pesos (dC/dw):\n",
            "          W(Area): 0.155359\n",
            "     W(Perimetro): 0.152671\n",
            "   W(Compactidad): 0.008949\n",
            "    W(LongNucleo): 0.058939\n",
            "   W(AnchoNucleo): 0.033201\n",
            "     W(Asimetria): 0.019981\n",
            "     W(LongSurco): 0.056221\n",
            "\n",
            "Gradiente del Sesgo (dC/db):\n",
            "  b: 0.010337\n"
          ]
        }
      ]
    }
  ]
}